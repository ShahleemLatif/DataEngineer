# Data Modeling Using Postgres
*by Shahleem Latif*

## Introduction

A startup called Sparkify wants to analyze the data they have been
collecting on songs and user activity on their new music streaming app.
The analytics team is particularly interested in understanding what
songs users are listening to. Currently, they do not have an easy way to
query their data, which resides in a directory of JSON logs on user
activity on the app, as well as a directory with JSON metadata on the
songs in their app.

They like a data engineer to create a Postgres database with tables
designed to optimize queries on song play analysis, and bring you on the
project. Your role is to create a database schema and ETL pipeline for
this analysis. You will be able to test your database and ETL pipeline by
running queries given to you by the analytics team from Sparkify and
compare your results with their expected results.

## Project Steps

### Step 1

 - Create sql_queries.py file in python and write queries to create
    artists, songplays, songs, time, and users tables
    
 - Then also write insert queries to insert data into the tables in the
    same file

###  Step 2

 - Create a database called "sparkifydb" in Postgres running
    create_tables.py file
    
 - Create four tables in "sparkifydb" database named artists,
    songplays, songs, time, and users in Postgres by running create_tables.py
    file

### Step 3

 - Create an Extract Transfer Load file using python and name it etl.py
 
 - ETL Process: Inserted data into the four tables using etl.py

### Step 4

 - To run the test, first create an etl.ipynb file in Jupyter similar to the
    etl.py file created in step 3
    
 - Create and run a test.ipynb file in Jupyter

## File Structure 

To get started with the project, go to the workspace on the next page, where you'll find the project template files. You can work on your project and submit your work through this workspace. Alternatively, you can download the project template files from the Resources folder if you'd like to develop your project locally.

In addition to the data files, the project workspace includes six files:

1. `sql_queries.py` contains all your sql queries, and is imported into the last three files above. 
2.  `create_tables.py` drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
3.  `etl.py` reads and processes files from `song_data`  and `log_data` and loads them into your tables. You can fill this out based on your work in the ETL notebook.
4. `etl.ipynb` reads and processes a single file from `song_data` and `log_data` and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
5. `test.ipynb` displays the first few rows of each table to let you check your database.
6. `README.md` provides discussion on your project.
  


## Datasets

There are two datasets that were used for this project.  The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/) in JSON format.  
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

```
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

```

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![](https://video.udacity-data.com/topher/2019/February/5c6c15e9_log-data/log-data.png)

## Database Schema, Tables and Queries

I installed Postgres 13, connected the python files to Postgres, and ran the python files on my local computer.  After running the python files I went to the command line and enter: 
**C:\Program Files\PostgreSQL\13\bin>pg_dump -U postgres -s sparkifydb > C:\data\sparkify_schema.sql**
**Password:**

**C:\Program Files\PostgreSQL\13\bin>**

This will create a schema file:
*Example*

### PostgreSQL database dump


Dumped from database version 13.0
Dumped by pg_dump version 13.0

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_table_access_method = heap;

### Name: artists; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.artists (
    artist_id character varying NOT NULL,
    name character varying,
    location character varying,
    latitude numeric,
    longitude numeric
);

ALTER TABLE public.artists OWNER TO postgres;

### Name: songplays; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.songplays (
    songplay_id integer NOT NULL,
    start_time bigint,
    user_id character varying,
    level character varying,
    song_id character varying,
    artist_id character varying,
    session_id character varying,
    location character varying,
    user_agent character varying
);

ALTER TABLE public.songplays OWNER TO postgres;

### Name: songplays_songplay_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres

CREATE SEQUENCE public.songplays_songplay_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;

ALTER TABLE public.songplays_songplay_id_seq OWNER TO postgres;

### Name: songplays_songplay_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres

ALTER SEQUENCE public.songplays_songplay_id_seq OWNED BY public.songplays.songplay_id;

### Name: songs; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.songs (
    song_id character varying NOT NULL,
    title character varying,
    artist_id character varying,
    year integer,
    duration double precision
);

ALTER TABLE public.songs OWNER TO postgres;

### Name: time; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public."time" (
    start_time timestamp without time zone NOT NULL,
    hour integer,
    day integer,
    week integer,
    month integer,
    year integer,
    weekday character varying
);

ALTER TABLE public."time" OWNER TO postgres;

### Name: users; Type: TABLE; Schema: public; Owner: postgres

CREATE TABLE public.users (
    user_id character varying NOT NULL,
    first_name character varying,
    last_name character varying,
    gender character varying,
    level character varying
);

ALTER TABLE public.users OWNER TO postgres;

### Name: songplays songplay_id; Type: DEFAULT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.songplays ALTER COLUMN songplay_id SET DEFAULT nextval('public.songplays_songplay_id_seq'::regclass);

### Name: artists artists_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.artists
    ADD CONSTRAINT artists_pkey PRIMARY KEY (artist_id);

### Name: songplays songplays_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.songplays
    ADD CONSTRAINT songplays_pkey PRIMARY KEY (songplay_id);

### Name: songs songs_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.songs
    ADD CONSTRAINT songs_pkey PRIMARY KEY (song_id);

### Name: time time_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public."time"
    ADD CONSTRAINT time_pkey PRIMARY KEY (start_time);

### Name: users users_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres

ALTER TABLE ONLY public.users
    ADD CONSTRAINT users_pkey PRIMARY KEY (user_id);

### PostgreSQL database dump complete


 










<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE4MjcxODEzNzldfQ==
-->